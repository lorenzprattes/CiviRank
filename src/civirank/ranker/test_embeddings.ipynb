{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706fd52653ef4ac8b8950bebafaca645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a19f12ede54892a41a7ea13c29f140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from analyzers import ProsocialityPolarizationAnalyzer\n",
    "import pandas as pd\n",
    "test = ProsocialityPolarizationAnalyzer(language=\"ger\")\n",
    "prosocial = [\n",
    "    \"Wow, das ist wirklich bewundernswert! Deine Perspektive bringt so viel Wert in diese Diskussion.\",\n",
    "    \"Danke, dass du dein Wissen teilst! Deine Ratschläge sind wirklich hilfreich und machen die Welt ein bisschen besser.\",\n",
    "    \"So eine umgängliche und respektvolle Art zu diskutieren – das sollte es viel öfter geben!\",\n",
    "    \"Ich spüre echte Zuneigung in deinen Worten. Es ist schön zu sehen, dass jemand sich so sehr für andere interessiert.\",\n",
    "    \"Dein Beitrag war wirklich angenehm zu lesen – voller positiver Vibes und guter Ideen!\",\n",
    "    \"Du hast mit deiner Hilfe hier echt etwas bewirkt. Danke für deine Zeit und Mühe!\",\n",
    "    \"Es ist schön zu sehen, wie Menschen sich für eine gemeinsame Sache engagieren. Respekt!\",\n",
    "    \"So viel Altruismus und Herzlichkeit – die Welt braucht mehr Menschen wie dich!\",\n",
    "    \"Eine wirklich liebenswürdige Geste! Das zeigt wahre Freundlichkeit und Mitgefühl.\",\n",
    "    \"Ich schätze deine Worte und die Gedanken, die du dir machst. Danke für deinen wertvollen Beitrag!\",\n",
    "    \"So eine zugängliche und herzliche Art macht dieses Forum zu einem angenehmen Ort.\",\n",
    "    \"Es ist inspirierend zu sehen, wie du andere unterstützt. Davon kann man sich echt eine Scheibe abschneiden!\",\n",
    "    \"Dein Humor ist einfach erfrischend – ein bisschen Scherzen tut immer gut!\",\n",
    "    \"Es ist großartig zu sehen, wie neue Freundschaften durch den Austausch hier entstehen.\",\n",
    "    \"Wohltätigkeit und Großzügigkeit wie deine sind nicht selbstverständlich – danke für dein Engagement!\",\n",
    "    \"Deine überparteiliche Art macht dich zu einer wertvollen Stimme in dieser Diskussion.\",\n",
    "    \"Es ist so bereichernd, sich kreativ auszutauschen. Danke für deine Ideen!\",\n",
    "    \"Echte Kameradschaft zeigt sich in schwierigen Zeiten – und du hast das bewiesen.\",\n",
    "    \"Du bringst so viel Fürsorge und Wärme in diese Community. Danke dafür!\",\n",
    "    \"Ein charitativer Gedanke, der andere inspiriert. Schön, dass es Menschen wie dich gibt.\",\n",
    "    \"Gemeinwohlorientiertes Denken wie deines macht einen echten Unterschied!\",\n",
    "    \"Kooperation statt Konfrontation – danke für deinen konstruktiven Ansatz!\",\n",
    "    \"Man spürt die kollegiale Atmosphäre, die du schaffst. Toll, dass du hier bist!\",\n",
    "    \"Mitgefühl und Verständnis sind selten im Internet – du bringst beides mit! Vielen Dank.\",\n",
    "    \"Es ist schön zu sehen, wie du anderen Mut machst. Deine Worte haben Kraft!\",\n",
    "    \"Ein so respektvoller und rücksichtsvoller Kommentar – genau das braucht das Internet mehr!\",\n",
    "    \"Gratulation zu deinem Erfolg! Du hast es dir wirklich verdient.\",\n",
    "    \"Du hast eine so höfliche und diplomatische Art, mit anderen umzugehen – das ist bewundernswert!\",\n",
    "    \"Deine Ermutigung macht sicher vielen Mut, sich ebenfalls einzubringen!\",\n",
    "    \"Danke für dein freundliches und wohlwollendes Engagement – das zeigt echte Menschlichkeit!\",\n",
    "    \"Eine großherzige Geste wie diese ist heutzutage nicht selbstverständlich – Respekt!\",\n",
    "    \"Ein sanfter und zugleich wirkungsvoller Beitrag – danke für deine Weisheit.\",\n",
    "    \"Deine Art zu führen ist inspirierend – mit Respekt, Teamgeist und Empathie!\",\n",
    "    \"Ein echter Lichtblick in dieser Diskussion. Danke für deinen positiven Einfluss!\",\n",
    "    \"Danke, dass du die Welt ein kleines bisschen besser machst – mit Worten und Taten.\"\n",
    "]\n",
    "polarized = [\n",
    "    \"Die ganzen Flüchtlinge sollten sofort abgeschoben werden!\",\n",
    "    \"Du bist so dumm, es ist unfassbar.\",\n",
    "    \"Abtreibung ist Mord! Wer das tut, gehört bestraft.\",\n",
    "    \"Diese Politiker sind alle korrupt und gehören verhaftet!\",\n",
    "    \"Keiner braucht diese linksextreme Propaganda!\",\n",
    "    \"Das ist kompletter Bullshit, was du da erzählst!\",\n",
    "    \"Die Medien sind doch nur Lügenpresse!\",\n",
    "    \"Diese Heuchler sollte man zur Verantwortung ziehen!\",\n",
    "    \"Das Gesundheitssystem ist ein Witz, danke an die Politiker!\",\n",
    "    \"Keiner braucht diese dreckigen Migranten hier!\",\n",
    "    \"Die Regierung manipuliert uns alle, merkt ihr das nicht?!\",\n",
    "    \"Alle Muslime sind Terroristen, das ist einfach Fakt.\",\n",
    "    \"Die ganze Klimawandel-Lüge ist nur ein Trick für mehr Steuern!\",\n",
    "    \"Jeder, der diesen Mist glaubt, ist ein kompletter Idiot.\",\n",
    "    \"Kapitalismus ist das schlimmste System überhaupt!\"\n",
    "]\n",
    "prosocial = pd.DataFrame(prosocial, columns=[\"text\"])\n",
    "polarization = pd.DataFrame(polarized, columns=[\"text\"])\n",
    "\n",
    "polarization.loc[:, \"prosociality\"] = test.get_similarity_prosocial(polarization)\n",
    "polarization.loc[:, \"polarization\"] = test.get_similarity_polarization(polarization)\n",
    "\n",
    "prosocial.loc[:, \"polarization\"] = test.get_similarity_polarization(prosocial)\n",
    "prosocial.loc[:, \"prosociality\"] = test.get_similarity_prosocial(prosocial)\n",
    "\n",
    "# prosocial.apply(lambda x: test.get_similarity_polarization(x))\n",
    "# polarization.apply(lambda x: test.get_similarity_polarization(x))\n",
    "# prosocial.apply(lambda x: test.get_similarity_prosocial(x))\n",
    "# polarization.apply(lambda x: test.get_similarity_prosocial(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575f08a05d6f4b4f82efa6b500eee681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cff4fa1ea24a04844f4ff9a447dcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments = {\n",
    "    \"polarizing\": [\n",
    "        \"Anyone who supports [Party X] is completely ignorant and ruining the country.\",\n",
    "        \"If you vote for [Candidate Y], you're either evil or brainwashed.\",\n",
    "        \"People who believe in [Topic Z] are the worst kind of people.\",\n",
    "        \"Anyone who thinks [controversial stance] should be banned from society.\",\n",
    "        \"If you’re not with us, you’re against us. No middle ground!\",\n",
    "        \"Only idiots believe in [Religion/Belief System].\",\n",
    "        \"The media is lying to you, and only WE know the truth!\",\n",
    "        \"Scientists are just pushing an agenda; don’t trust them!\"\n",
    "    ],\n",
    "    \"non_polarizing\": [\n",
    "        \"Different policies have different pros and cons, and we should discuss them rationally.\",\n",
    "        \"I understand why some people support [Policy X], but I prefer [Policy Y] for these reasons.\",\n",
    "        \"People have different perspectives, and it’s important to listen to each other.\",\n",
    "        \"Let’s find a compromise that respects everyone’s concerns.\",\n",
    "        \"I see your point, but I respectfully disagree because...\",\n",
    "        \"That’s an interesting perspective! I think differently, but I appreciate the discussion.\",\n",
    "        \"At the end of the day, we all want what’s best for our community.\",\n",
    "        \"There’s more that unites us than divides us.\"\n",
    "    ],\n",
    "    \"moderate_polarizing\": [\n",
    "        \"I’m really tired of seeing [policy] being pushed, it’s not working.\",\n",
    "        \"I don’t agree with [group], but they have a right to their opinion.\",\n",
    "        \"Are we really sure that [Policy/Science Claim] is the best approach?\",\n",
    "        \"The media covers this topic in a very one-sided way.\",\n",
    "        \"I strongly disagree with [viewpoint], and here’s why...\",\n",
    "        \"We should be more critical of [institution], but that doesn’t mean they’re all bad.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "test_eng = ProsocialityPolarizationAnalyzer(language=\"en\")\n",
    "polarizing = pd.DataFrame(comments['polarizing'], columns=[\"text\"])\n",
    "non_polarizing = pd.DataFrame(comments['non_polarizing'], columns=[\"text\"])\n",
    "\n",
    "polarizing.loc[:, \"prosociality\"] = test_eng.get_similarity_prosocial(polarizing)\n",
    "polarizing.loc[:, \"polarization\"] = test_eng.get_similarity_polarization(polarizing)\n",
    "\n",
    "non_polarizing.loc[:, \"polarization\"] = test_eng.get_similarity_polarization(non_polarizing)\n",
    "non_polarizing.loc[:, \"prosociality\"] = test_eng.get_similarity_prosocial(non_polarizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03479303  0.02635196 -0.044272   ...  0.01743882 -0.01952394\n",
      "  -0.00118101]\n",
      " [ 0.02096636 -0.00401741 -0.0509371  ...  0.03618196  0.02944081\n",
      "  -0.04497767]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Ich bin heute sehr glücklich! Aber ih schriebe auch ganz viel dazu um mein embedding zu verwirren, ha wie gefällt dir das?' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.1530\n",
      "\n",
      "Text: 'Es ist ein trauriger und düsterer Tag.' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.0658\n",
      "\n",
      "Text: 'Lächeln und Lachen machen mich fröhlich.' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.4105\n",
      "\n",
      "Text: 'Du hast eine so höfliche und diplomatische Art, mit anderen umzugehen – das ist bewundernswert!' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: -0.0710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained multilingual sentence transformer model (supports German)\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "\n",
    "# Load spaCy model for German NLP tasks\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "# Define a dictionary related to \"happiness\" in German\n",
    "happiness_dict_german = [\"Freude\", \"Glück\", \"Lächeln\", \"Vergnügen\", \"fröhlich\", \"zufrieden\"]\n",
    "\n",
    "# Compute embeddings for dictionary words and average them\n",
    "dict_embeddings = model.encode(happiness_dict_german, convert_to_tensor=True)\n",
    "avg_dict_embedding = dict_embeddings.mean(dim=0)\n",
    "\n",
    "# Example German texts to analyze\n",
    "texts = [\n",
    "    \"Ich bin heute sehr glücklich! Aber ih schriebe auch ganz viel dazu um mein embedding zu verwirren, ha wie gefällt dir das?\",\n",
    "    \"Es ist ein trauriger und düsterer Tag.\",\n",
    "    \"Lächeln und Lachen machen mich fröhlich.\",\n",
    "        \"Du hast eine so höfliche und diplomatische Art, mit anderen umzugehen – das ist bewundernswert!\",\n",
    "]\n",
    "\n",
    "# Compute similarity between each text and the dictionary representation\n",
    "for text in texts:\n",
    "    text_embedding = model.encode(text, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(text_embedding, avg_dict_embedding).item()\n",
    "    print(f\"Text: '{text}' \\nÄhnlichkeit zur Glücklichkeits-Wörterbuch: {similarity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aufnehmen', 'bewundern', 'beraten', 'umgänglich', 'zuneigung', 'angenehm', 'helfen', 'bündnis', 'altruismus', 'liebenswürdig', 'freundschaftlich', 'entschuldigen', 'wertschätzen', 'zugänglich', 'unterstützen', 'scherzen', 'anfreunden', 'wohltätig', 'gütig', 'großherzig', 'überparteilich', 'kreativ austauschen', 'gemeinsame sache', 'kameradschaft', 'fürsorge', 'charitativ', 'gemeinwohlorientiert', 'kooperativ', 'koalition', 'zusammenarbeiten', 'kollegial', 'mitfühlen', 'gesellig', 'mitgefühl', 'kompliment', 'kompromiss', 'kameradschaft', 'beileid', 'sympathisch', 'gratulieren', 'rücksichtsvoll', 'trösten', 'beitragen', 'kooperieren', 'koordinieren', 'höflichkeit', 'team', 'engagiert', 'verlässlich', 'diplomatisch', 'wohltätig', 'spenden', 'spender', 'empathie', 'ermutigen', 'befürworten', 'väterlich', 'gefallen', 'zugetan', 'vergeben', 'freundlich', 'freundschaft', 'großzügig', 'herzlich', 'sanftmut', 'geschenk', 'gutherzig', 'gutmütig', 'wohlgesinnt', 'gnädig', 'dankbarkeit', 'gesellig', 'gruppendenken', 'führen', 'helfen', 'abklatschen', 'gastfreundlich', 'menschlich', 'humanitär', 'inklusiv', 'einladen', 'warmherzig', 'freundlich', 'freundlichkeit', 'vermittlung', 'liebevoll', 'loyal', 'großmütig', 'vermitteln', 'barmherzigkeit', 'mütterlich', 'nachbarschaftlich', 'pflegen', 'fördern', 'anbieten', 'aufgeschlossen', 'partner', 'geduldig', 'friedensstiftend', 'philanthropisch', 'gefallen', 'höflich', 'loben', 'prosozial', 'hinweis', 'verbindung', 'gegenseitig', 'versöhnen', 'retten', 'lösung', 'respekt', 'selbstlos', 'aufrichtig', 'sozialisieren', 'weichherzig', 'beruhigen', 'leid tun', 'truppe', 'unterstützen', 'mitfühlend', 'synergetisch', 'team', 'zartbesaitet', 'sanftmütig', 'danken', 'waffenstillstand', 'selbstlos', 'ehrenamtlich', 'warmherzig', 'zuvorkommend', 'verbindend', 'einfühlsam', 'beistehen', 'verständnisvoll', 'ermutigend', 'aufopferungsvoll', 'harmonisch', 'gemeinsinnig', 'höflichkeitsvoll', 'wohlwollend', 'hilfsbereit', 'selbstlos handeln', 'gemeinschaftlich', 'nachsichtig', 'tröstlich', 'verantwortungsbewusst', 'respektvoll', 'gütmütig', 'kameradschaftlich', 'hilfreich', 'ermutigend', 'erleichternd', 'förderlich']\n",
      "Text: 'Ich bin heute sehr glücklich!' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.0643\n",
      "\n",
      "Text: 'Es ist ein trauriger und düsterer Tag.' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.0160\n",
      "\n",
      "Text: 'Lächeln und Lachen machen mich fröhlich.' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.0454\n",
      "\n",
      "Text: 'Danke für dein freundliches und wohlwollendes Engagement – das zeigt echte Menschlichkeit!' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.0426\n",
      "\n",
      "Text: 'Eine großherzige Geste wie diese ist heutzutage nicht selbstverständlich – Respekt!' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: -0.0286\n",
      "\n",
      "Text: 'Ein sanfter und zugleich wirkungsvoller Beitrag – danke für deine Weisheit.' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.0688\n",
      "\n",
      "Text: 'Deine Art zu führen ist inspirierend – mit Respekt, Teamgeist und Empathie!' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.0279\n",
      "\n",
      "Text: 'Ein echter Lichtblick in dieser Diskussion. Danke für deinen positiven Einfluss!' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: -0.0282\n",
      "\n",
      "Text: 'Danke, dass du die Welt ein kleines bisschen besser machst – mit Worten und Taten.' \n",
      "Ähnlichkeit zur Glücklichkeits-Wörterbuch: 0.0510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Load pre-trained multilingual sentence transformer model (supports German)\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "\n",
    "# Load spaCy model for German NLP tasks\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "# Define a dictionary related to \"happiness\" in German\n",
    "parent_dir = Path(Path.cwd()).resolve().parent\n",
    "fname = \"prosocial_dictionary_\" + \"ger\" + \".csv\"\n",
    "filepath = parent_dir / 'data' / fname\n",
    "if not os.path.exists(filepath):\n",
    "    raise FileNotFoundError(f\"The specified path to prosocial dictionary '{filepath}' does not exist.\")\n",
    "prosocial_dict = pd.read_csv(filepath, header=None, names = ['word'])\n",
    "prosocial_dict[\"word\"] = prosocial_dict[\"word\"].str.replace(\"*\", \"\")\n",
    "prosocial_dict = list(prosocial_dict[\"word\"].values)\n",
    "print(prosocial_dict)\n",
    "# Compute embeddings for dictionary words and average them\n",
    "dict_embeddings = model.encode(prosocial_dict, convert_to_tensor=True)\n",
    "avg_dict_embedding = dict_embeddings.mean(dim=0)\n",
    "\n",
    "# Example German texts to analyze\n",
    "texts = [\n",
    "    \"Ich bin heute sehr glücklich!\",\n",
    "    \"Es ist ein trauriger und düsterer Tag.\",\n",
    "    \"Lächeln und Lachen machen mich fröhlich.\",\n",
    "    \"Danke für dein freundliches und wohlwollendes Engagement – das zeigt echte Menschlichkeit!\",\n",
    "    \"Eine großherzige Geste wie diese ist heutzutage nicht selbstverständlich – Respekt!\",\n",
    "    \"Ein sanfter und zugleich wirkungsvoller Beitrag – danke für deine Weisheit.\",\n",
    "    \"Deine Art zu führen ist inspirierend – mit Respekt, Teamgeist und Empathie!\",\n",
    "    \"Ein echter Lichtblick in dieser Diskussion. Danke für deinen positiven Einfluss!\",\n",
    "    \"Danke, dass du die Welt ein kleines bisschen besser machst – mit Worten und Taten.\"\n",
    "]\n",
    "\n",
    "# Compute similarity between each text and the dictionary representation\n",
    "for text in texts:\n",
    "    text_embedding = model.encode(text, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(text_embedding, avg_dict_embedding).item()\n",
    "    print(f\"Text: '{text}' \\nÄhnlichkeit zur Glücklichkeits-Wörterbuch: {similarity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class testProsocialityPolarizationAnalyzer():\n",
    "    '''\n",
    "        Class that loads a model to compute the similarity of a text to a prosociality and a polarization dictionary. The similarity is computed as the cosine similarity between the text embeddings and the dictionary embeddings. \n",
    "\n",
    "        # Polarization\n",
    "        Class that loads pre-calculated embeddings of the affective polarization dictionary from  https://academic.oup.com/pnasnexus/article/1/1/pgac019/6546199?login=false#381342977 and calculates similar embeddings using GloVe for a given text. It exposes a function get_similarity_polarization() that calculates the cosine similarity between the averaged dictionary embeddings and the text embedding following the DDR approach (see https://doi.org/10.3758/s13428-017-0875-9). The function returns a single floating point value between -1 and+1, with values closer to -1 meaning a text is less similar to polarizing language whereas values closer to +1 are more similar to polarizing language.\n",
    "\n",
    "        # Prosociality\n",
    "        Similar to the polarization class, it loads a dictionary of prosocial terms and calculates the cosine similarity between the averaged dictionary embeddings and the text embeddings. The function get_similarity_prosocial() returns a single floating point value between -1 and +1, with values closer to -1 meaning a text is less similar to prosocial language whereas values closer to +1 are more similar to prosocial language.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model_id = 'joaopn/glove-model-reduced-stopwords', label_filter = 'issue', language=\"en\"):\n",
    "        self.language = language\n",
    "        self.model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "        self.nlp = spacy.load(\"de_core_news_md\")\n",
    "        self.label_filter = label_filter\n",
    "        # Load terms and compute their embeddings\n",
    "        self.load_prosocial()\n",
    "        self.load_polarization()\n",
    "\n",
    "\n",
    "    def load_prosocial(self):\n",
    "        # Load terms from CSV\n",
    "        parent_dir = Path(Path.cwd()).resolve().parent\n",
    "        fname = \"prosocial_dictionary_\" + self.language + \".csv\"\n",
    "        filepath = parent_dir / 'data' / fname\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"The specified path to prosocial dictionary '{filepath}' does not exist.\")\n",
    "        prosocial_dict = pd.read_csv(filepath, header=None, names = ['word'])\n",
    "        prosocial_dict[\"word\"] = prosocial_dict[\"word\"].str.replace(\"*\", \"\")\n",
    "        prosocial_dict = list(prosocial_dict[\"word\"].values)\n",
    "\n",
    "        # Compute embeddings for the unique words\n",
    "        self.dict_embeddings = self.model.encode(\n",
    "            prosocial_dict,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "\n",
    "        # Average the embeddings to create a single dictionary embedding\n",
    "        self.dict_embeddings_prosocial = torch.mean(self.dict_embeddings, dim=0)\n",
    "\n",
    "    def load_polarization(self):\n",
    "        # Load terms from CSV\n",
    "        parent_dir = Path(Path.cwd()).resolve().parent\n",
    "        fname = \"polarization_dictionary_\" + self.language + \".csv\"\n",
    "        filepath = parent_dir /'data' / fname\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"The specified path to polarization dictionary '{filepath}' does not exist.\")\n",
    "        df = pd.read_csv(filepath, header=0)\n",
    "        if self.label_filter is not None:\n",
    "            df = df[df['label'] == self.label_filter]\n",
    "        unique_words = df['word'].unique()\n",
    "\n",
    "        # Compute embeddings for the unique words\n",
    "        self.dict_embeddings = self.model.encode(\n",
    "            list(unique_words),\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "\n",
    "        # Average the embeddings to create a single dictionary embedding\n",
    "        self.dict_embeddings_polarization = torch.mean(self.dict_embeddings, dim=0)\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        def pre_text(text):\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"(?:https?://[^\\s]+)\", \"\", text)\n",
    "            text = re.sub(r\"&.*;\", \"\", text)\n",
    "            text = re.sub(r\"[\\t\\n\\r]+\", \" \", text)\n",
    "            text = re.sub(r\"@\\w+\", \"@user\", text)\n",
    "            tokens = [token.lemma_ for token in nlp(text) if not token.is_stop]\n",
    "            return \" \".join(tokens)\n",
    "        df[\"text\"] = df[\"text\"].apply(pre_text)\n",
    "        \n",
    "    def get_embeddings(self, df):\n",
    "        # Encode text in batches\n",
    "        corpus_embeddings = self.model.encode(\n",
    "            list(df[\"text\"]),\n",
    "            show_progress_bar=False,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        assert len(corpus_embeddings) == len(df)\n",
    "        return corpus_embeddings\n",
    "\n",
    "    def get_similarity_prosocial(self, texts):\n",
    "        df = texts.copy()\n",
    "        self.preprocess(df)\n",
    "        text_embeddings = self.get_embeddings(df)\n",
    "        cos_sim = util.cos_sim(text_embeddings, self.dict_embeddings_prosocial)\n",
    "        return cos_sim.cpu().numpy()\n",
    "\n",
    "    def get_similarity_polarization(self, texts):\n",
    "        df = texts.copy()\n",
    "        self.preprocess(df)\n",
    "        text_embeddings = self.get_embeddings(df)\n",
    "        cos_sim = util.cos_sim(text_embeddings, self.dict_embeddings_polarization)\n",
    "        return cos_sim.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cf5f5cdf80446dac1cf454e1869454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cac50b4e55e4c56bd9427ffa74a1f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_class = testProsocialityPolarizationAnalyzer(language=\"ger\")\n",
    "# Example German texts to analyze for polarization\n",
    "texts = [\n",
    "    \"Diese Nachricht ist pure Manipulation und Hetze gegen unsere Gemeinschaft!\",\n",
    "    \"Wir müssen zusammenhalten und die Wahrheit verbreiten. #GemeinsamStark\",\n",
    "    \"@user Die Regierung verbreitet nur Lügen und Fake News! Schau hier: http://link.de\",\n",
    "    \"Radikale Ansichten führen nur zur Spaltung der Gesellschaft.\"\n",
    "]\n",
    "texts = pd.DataFrame(texts, columns=[\"text\"])\n",
    "\n",
    "texts.loc[:, \"prosociality\"] = test_class.get_similarity_prosocial(texts)\n",
    "texts.loc[:, \"polarization\"] = test_class.get_similarity_polarization(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Diese Nachricht ist pure Manipulation und Hetze gegen unsere Gemeinschaft.' \n",
      "Ähnlichkeit zur Polarisierungs-Wörterbuch: 0.1432\n",
      "\n",
      "Text: 'Wir müssen zusammenhalten und die Wahrheit verbreiten.' \n",
      "Ähnlichkeit zur Polarisierungs-Wörterbuch: 0.1075\n",
      "\n",
      "Text: 'Die Regierung verbreitet nur Lügen und Fake News!' \n",
      "Ähnlichkeit zur Polarisierungs-Wörterbuch: 0.2031\n",
      "\n",
      "Text: 'Radikale Ansichten führen nur zur Spaltung der Gesellschaft.' \n",
      "Ähnlichkeit zur Polarisierungs-Wörterbuch: 0.1281\n",
      "\n",
      "Text: 'Danke, dass du die Welt ein kleines bisschen besser machst – mit Worten und Taten.' \n",
      "Ähnlichkeit zur Polarisierungs-Wörterbuch: -0.0269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load German sentence transformer model\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "\n",
    "# Load spaCy model for German NLP processing\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "# Define a dictionary of polarized words in German\n",
    "polarization_dict_german = [\n",
    "    \"Lüge\", \"Hass\", \"Hetze\", \"Propaganda\", \"Feind\", \"Manipulation\", \"Extremismus\", \n",
    "    \"Fake News\", \"Spaltung\", \"Radikal\", \"Verschwörung\", \"Indoktrination\"\n",
    "]\n",
    "\n",
    "# Compute embeddings for dictionary words and average them\n",
    "dict_embeddings = model.encode(polarization_dict_german, convert_to_tensor=True)\n",
    "avg_dict_embedding = dict_embeddings.mean(dim=0)\n",
    "\n",
    "# Example German texts to analyze for polarization\n",
    "texts = [\n",
    "    \"Diese Nachricht ist pure Manipulation und Hetze gegen unsere Gemeinschaft.\",\n",
    "    \"Wir müssen zusammenhalten und die Wahrheit verbreiten.\",\n",
    "    \"Die Regierung verbreitet nur Lügen und Fake News!\",\n",
    "    \"Radikale Ansichten führen nur zur Spaltung der Gesellschaft.\",\n",
    "    \"Danke, dass du die Welt ein kleines bisschen besser machst – mit Worten und Taten.\"\n",
    "]\n",
    "\n",
    "# Compute similarity between each text and the polarization dictionary\n",
    "for text in texts:\n",
    "    text_embedding = model.encode(text, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(text_embedding, avg_dict_embedding).item()\n",
    "    print(f\"Text: '{text}' \\nÄhnlichkeit zur Polarisierungs-Wörterbuch: {similarity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 'Diese Nachricht ist pure Manipulation und Hetze gegen unsere Gemeinschaft!' \n",
      "Gereinigter Text: 'nachricht pure manipulation hetze gemeinschaft'\n",
      "Ähnlichkeit zur Polarisierungs-Wörterbuch: 0.2593\n",
      "\n",
      "Original Text: 'Wir müssen zusammenhalten und die Wahrheit verbreiten. #GemeinsamStark' \n",
      "Gereinigter Text: 'zusammenhalten wahrheit verbreiten gemeinsamstark'\n",
      "Ähnlichkeit zur Polarisierungs-Wörterbuch: 0.3212\n",
      "\n",
      "Original Text: '@user Die Regierung verbreitet nur Lügen und Fake News! Schau hier: http://link.de' \n",
      "Gereinigter Text: 'user regierung verbreitet lügen fake news schau'\n",
      "Ähnlichkeit zur Polarisierungs-Wörterbuch: 0.2717\n",
      "\n",
      "Original Text: 'Radikale Ansichten führen nur zur Spaltung der Gesellschaft.' \n",
      "Gereinigter Text: 'radikale ansichten führen spaltung gesellschaft'\n",
      "Ähnlichkeit zur Polarisierungs-Wörterbuch: 0.2775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load German NLP model\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "# Load German sentence transformer model\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "\n",
    "# Define a dictionary of polarized words in German\n",
    "polarization_dict_german = [\n",
    "    \"Lüge\", \"Hass\", \"Hetze\", \"Propaganda\", \"Feind\", \"Manipulation\", \"Extremismus\", \n",
    "    \"Fake News\", \"Spaltung\", \"Radikal\", \"Verschwörung\", \"Indoktrination\"\n",
    "]\n",
    "\n",
    "# Compute embeddings for dictionary words and average them\n",
    "dict_embeddings = model.encode(polarization_dict_german, convert_to_tensor=True)\n",
    "avg_dict_embedding = dict_embeddings.mean(dim=0)\n",
    "\n",
    "# Function to preprocess text (remove URLs, punctuation, stopwords, etc.)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"@user\", text)  # Replace usernames\n",
    "    text = re.sub(r\"[^\\w\\säöüß]\", \"\", text)  # Remove punctuation (except German characters)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "\n",
    "    # Process text with spaCy and remove stopwords\n",
    "    doc = nlp(text)\n",
    "    filtered_text = \" \".join([token.text for token in doc if token.text not in nlp.Defaults.stop_words])\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "# Example German texts to analyze for polarization\n",
    "texts = [\n",
    "    \"Diese Nachricht ist pure Manipulation und Hetze gegen unsere Gemeinschaft!\",\n",
    "    \"Wir müssen zusammenhalten und die Wahrheit verbreiten. #GemeinsamStark\",\n",
    "    \"@user Die Regierung verbreitet nur Lügen und Fake News! Schau hier: http://link.de\",\n",
    "    \"Radikale Ansichten führen nur zur Spaltung der Gesellschaft.\"\n",
    "]\n",
    "\n",
    "# Compute similarity between each text and the polarization dictionary\n",
    "for text in texts:\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    text_embedding = model.encode(cleaned_text, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(text_embedding, avg_dict_embedding).item()\n",
    "    print(f\"Original Text: '{text}' \\nGereinigter Text: '{cleaned_text}'\\nÄhnlichkeit zur Polarisierungs-Wörterbuch: {similarity:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
